{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Vanilla DQN or a Dueling DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOAL:** To train a _Vanilla Deep Q-Network_ or a _Dueling Deep Q-Network_ to play Connect4. The same notebook can be used to train both of them. This is the second part of our training pipeline (the second part also includes other RL algorithms):\n",
    "  - **Part 1) Supervised Learning**\n",
    "    - refer to *'src/train/part1_supervised_learning.ipynb'*.\n",
    "    - RESULT: a pre-trained network with basic knowledge of the game\n",
    "  - **Part 2) Reinforcement Learning**\n",
    "    - In this case: Vanilla and Dueling DQNs\n",
    "    - **TRANFER LEARNING FROM PART 1:**\n",
    "      - Load the pre-trained weights from Part 1\n",
    "      - Freeze the convolutional block (*feature extractor*, it is not trained here)\n",
    "      - Train the rest of Fully Connected to estimate the optimal Q-values\n",
    "<br>\n",
    "                     \n",
    "**METHOD:**\n",
    "   - We implemented the *Minimax DQN* algorithm for turn-based games\n",
    "   - We used an *Experience Replay Memory* to break the correlation between consecutive samples\n",
    "       - capacity = 60k\n",
    "       - minimum size = 30k\n",
    "       - exponent for reward backpropagation = 3\n",
    "       - for more details on the implementation refer to '*src/data/replay_memory.py*'\n",
    "   - We used a *Target Network* to avoid overestimating the Q-values.\n",
    "   - The network architecture we used is defined in '*src/models/architectures/cnet128.json*'\n",
    "   - We applied *transfer learning* to use the knowledge learned in '*src/train/part1_supervised_learning.ipynb*'\n",
    "       - 1. load the network weights from '*src/models/saved_models/network_128.pt*'\n",
    "       - 2. freeze the convolutional block (*feature extractor*)\n",
    "       - 3. train the fully-connected layers to learn the Q-values\n",
    "   - We used a *Decaying epsilon-greedy* scheme to ensure sufficient exploration:\n",
    "       - epsilon start = 0.8\n",
    "       - epsilon decay = 600\n",
    "       - epislon end = 0.05\n",
    "   - There is an '*old agent*' that is an older and stable version of the agent. It is updated when:\n",
    "       - the agent achieve a new best win rate against the 1-StepLA Agent\n",
    "   - When the performance of the current network decreases significantly, the latest changes are undone and it goes back to the most recent *old weights*\n",
    "    \n",
    "    \n",
    "**TRAINING:**\n",
    "   - We trained for 100k time steps\n",
    "   - The learning hyperparameters are:\n",
    "       - learning rate = 1e-4\n",
    "       - batch size = 48\n",
    "       - weight decay (L2 regularization) = 5e-4\n",
    "       - discount factor (gamma) = 0.95\n",
    "       - 20 updates per new training episode\n",
    "       - loss function = Mean Squared Error (MSE) loss function\n",
    "       - update target network every 400 updates\n",
    "   - Every 1000 updates, the DQNagent competes against:\n",
    "       - vs the Random Agent\n",
    "       - vs the older network\n",
    "       - vs the 1-Step Lookahead Agent\n",
    "       \n",
    "**Vanilla DQN RESULTS:**\n",
    "   - Our best DQNAgent beats the 1-Step LookAhead Agent **≈87%** of the time\n",
    "   - The weights of the model are saved in '*src/models/saved_models/best_dqn.pt*'\n",
    "   - The training hyperaparameters are saved in '*src/models/saved_models/best_dqn_hparams.json*'\n",
    "   - Plots of the training losses\n",
    "   - Plots of the average game length in self-play games\n",
    "   - Plots of the evolution of the win rate vs 1StepLA\n",
    "   \n",
    "**Dueling DQN RESULTS:**\n",
    "   - Our best DuelingDQNAgent beats the 1-Step LookAhead Agent **≈94%** of the time\n",
    "   - The weights of the model are saved in '*src/models/saved_models/best_dueling_dqn.pt*'\n",
    "   - The training hyperaparameters are saved in '*src/models/saved_models/best_dueling_dqn_hparams.json*'\n",
    "   - Plots of the training losses\n",
    "   - Plots of the average game length in self-play games\n",
    "   - Plots of the evolution of the win rate vs 1StepLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PATH HERE\n",
    "code_dir = '/home/marc/Escritorio/RL-connect4/'\n",
    "\n",
    "if os.path.isdir(code_dir):\n",
    "    # local environment\n",
    "    os.chdir(code_dir)\n",
    "    print(f\"directory -> '{code_dir }'\")\n",
    "else:\n",
    "    # google colab environment\n",
    "    if os.path.isdir('./src'):\n",
    "        print(\"'./src' dir already exists\")\n",
    "    else:  # not unzipped yet\n",
    "        !unzip -q src.zip\n",
    "        print(\"'./src.zip' file successfully unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.baselines.random_agent import RandomAgent\n",
    "from src.agents.baselines.n_step_lookahead_agent import NStepLookaheadAgent\n",
    "from src.models.custom_network import CustomNetwork\n",
    "from src.agents.trainable.dqn_agent import DQNAgent\n",
    "from src.agents.trainable.dueling_dqn_agent import DuelingDQNAgent\n",
    "from src.environment.connect_game_env import ConnectGameEnv\n",
    "from src.data.replay_memory import ReplayMemory\n",
    "from src.eval.competition import competition\n",
    "from src.environment.env_utils import get_illegal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%d%m_%H%M\")\n",
    "print(f\"'{timestamp}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODELS = False  # if False, it is debug mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # Agent type (Vanilla DQN or DuelingDQN)\n",
    "    'is_dueling_dqn': True,\n",
    "    \n",
    "    # environment, data, memory\n",
    "    'memory_capacity': 60000,\n",
    "    'min_memory_size': 30000,\n",
    "    'reward_backprop_exponent': 3,\n",
    "    \n",
    "    # agent properties and model architecture\n",
    "    'avg_symmetric_q_vals': True,\n",
    "    'model_arch_path': './src/models/architectures/cnet128.json',\n",
    "    'pretrained_model_weights': './src/models/saved_models/supervised_cnet128.pt',\n",
    "    'freeze_conv_block': True,\n",
    "    \n",
    "    # Information displayed while training\n",
    "    'loss_log_every': 200,\n",
    "    'comp_every': 1000,\n",
    "    'vs_1StepLA_win_rate_decrease_to_undo_updates': 0.08,\n",
    "        \n",
    "    # Exploration Scheme (epsilon decay)\n",
    "    'eps_start': 0.8,\n",
    "    'eps_end': 0.05,\n",
    "    'eps_decay': 600,\n",
    "    \n",
    "    # Training loop params\n",
    "    'num_steps': 100000,\n",
    "    'batch_size': 48,\n",
    "    'gamma' : 0.95,\n",
    "    'weight_decay': 5e-4,\n",
    "    'lr': 1e-4,\n",
    "    'n_updates_per_new_episode': 20,\n",
    "\n",
    "    # target network update\n",
    "    'target_net_update_every': 400\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(from_: nn.Module, to_: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Copies the weights from the module 'from_' to the module 'to_'\n",
    "    Loading the state_dict from a module does not keep the\n",
    "    convolutional_block frozen, so it has to be done manually.\n",
    "    \"\"\"\n",
    "\n",
    "    to_.load_state_dict(from_.state_dict())\n",
    "    if hparams['freeze_conv_block']:\n",
    "        for param in to_.conv_block.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "            \n",
    "\n",
    "def create_model() -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create a DQN (or DuelingDQN) following the architecture in 'model_arch_file',\n",
    "    and initializing the network with the weights stored in 'load_weights_path'. \n",
    "    If it is DQN (not Dueling), remove the second prediction head \n",
    "    since it is not used.\n",
    "    \"\"\"\n",
    "\n",
    "    q_net = CustomNetwork.from_architecture(\n",
    "        file_path=hparams['model_arch_path']\n",
    "    ).to(device)\n",
    "\n",
    "    q_net.load_weights(hparams['pretrained_model_weights'])\n",
    "    \n",
    "    for param in q_net.conv_block.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    if not hparams['is_dueling_dqn']:\n",
    "        q_net.second_head = nn.Sequential()\n",
    "    \n",
    "    return q_net\n",
    "\n",
    "\n",
    "def create_agent():\n",
    "    \"\"\"\n",
    "    Create a new DQN (or DuelingDQN) Agent\n",
    "    \"\"\"\n",
    "\n",
    "    model_ = create_model()\n",
    "    \n",
    "    if hparams['is_dueling_dqn']: \n",
    "        agent_ = DuelingDQNAgent(\n",
    "            model=model_,\n",
    "            avg_symmetric_q_vals=hparams['avg_symmetric_q_vals'],\n",
    "        )\n",
    "    else:\n",
    "        agent_ = DQNAgent(\n",
    "            model=model_,\n",
    "            avg_symmetric_q_vals=hparams['avg_symmetric_q_vals'],\n",
    "        )\n",
    "\n",
    "    return agent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = create_agent()\n",
    "\n",
    "print(\"model device is cuda?\", next(dqn_agent.model.parameters()).is_cuda)\n",
    "print()\n",
    "print(summary(dqn_agent.model, input_size=dqn_agent.model.input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = dqn_agent.name.replace(' ', '_')\n",
    "save_best_vs_1StepLA_file = f'{agent_name}_'+'{win_rate}_vs_1StepLA_'+f'{timestamp}.pt'\n",
    "\n",
    "print('\"' + save_best_vs_1StepLA_file + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dqn_agent = create_agent()\n",
    "load_state_dict(from_=dqn_agent.model, to_=old_dqn_agent.model)\n",
    "old_dqn_agent.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net = create_model()\n",
    "load_state_dict(from_=dqn_agent.model, to_=target_net)\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Experience Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(\n",
    "    capacity=hparams['memory_capacity'],\n",
    "    reward_backprop_exponent=hparams['reward_backprop_exponent']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Prepare the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=dqn_agent.model.parameters(), \n",
    "    lr=hparams['lr'],\n",
    "    weight_decay=hparams['weight_decay']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eps_threshold(episode: int) -> int:\n",
    "    \"\"\"\n",
    "    Implements the decaying epsilon scheme.\n",
    "    Returns the exploration rate for the given episode number.\n",
    "    \"\"\"\n",
    "\n",
    "    return (hparams['eps_end'] + (hparams['eps_start'] - hparams['eps_end']) \n",
    "            * math.exp(-1. * episode / hparams['eps_decay']))\n",
    "\n",
    "\n",
    "# Plot the exploration rate for each training episode\n",
    "num_episodes = hparams['num_steps'] // hparams['n_updates_per_new_episode']\n",
    "episodes_x = range(num_episodes)\n",
    "eps_y = [compute_eps_threshold(x) for x in episodes_x]\n",
    "\n",
    "plt.plot(episodes_x, eps_y)\n",
    "plt.axhline(hparams['eps_start'], linestyle='--', alpha=0.4)\n",
    "plt.axhline(hparams['eps_end'], linestyle='--', alpha=0.4)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel('eps')\n",
    "plt.xlabel('training episode')\n",
    "plt.title('decaying epsilon scheme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_vals(model_: nn.Module, state_batch_: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Compute the Q-vals for the given batch of states.\n",
    "    This function accepts both DQN and DuelingDQN models\n",
    "    NOTE: output contains gradients since it is a prediction to learn from\n",
    "    \"\"\"\n",
    "\n",
    "    if hparams['is_dueling_dqn']:\n",
    "        adv, v = model_(state_batch_)\n",
    "        q_vals = v + (adv - adv.mean(dim=1).unsqueeze(-1))\n",
    "    else:\n",
    "        q_vals = model_(state_batch_)\n",
    "\n",
    "    return q_vals\n",
    "\n",
    "\n",
    "def compute_next_q_vals(target_net_: nn.Module, next_state_batch_: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Compute the Q-vals for the given batch of next states.\n",
    "    This function accepts both DQN and DuelingDQN models\n",
    "    NOTE: output does not contain gradient, it is part of the target\n",
    "    It can apply symmetric average.\n",
    "    It filters out illegal actions in next states (-inf Q-val)\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_vals = compute_q_vals(model_=target_net_,\n",
    "                                     state_batch_=next_state_batch_)\n",
    "\n",
    "    if hparams['avg_symmetric_q_vals']:\n",
    "        sym_next_state_batch = torch.flip(next_state_batch_, dims=[-1])\n",
    "        with torch.no_grad():\n",
    "            sym_next_q_vals = compute_q_vals(\n",
    "                model_=target_net_, state_batch_=sym_next_state_batch\n",
    "            )\n",
    "        next_q_vals = (next_q_vals + sym_next_q_vals.flip(dims=[1])) / 2\n",
    "\n",
    "    return next_q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(online_net_, \n",
    "                  target_net_, \n",
    "                  optimizer_, \n",
    "                  loss_func_,\n",
    "                  memory_, \n",
    "                  batch_size_, \n",
    "                  gamma_):\n",
    "    \n",
    "    if len(memory_) < hparams['min_memory_size']:\n",
    "        raise Exception(\"len(memory) is below its minimum value\")\n",
    "    \n",
    "    online_net_.train()\n",
    "\n",
    "    # get a batch of transitions (copy)\n",
    "    transitions = copy.deepcopy(memory_.sample(batch_size_))\n",
    "    batch = memory_.Transition(*zip(*transitions))\n",
    "    \n",
    "    # preprocess the states to feed the model\n",
    "    tuple_state_batch = tuple([online_net_.obs_to_model_input(obs=s)\n",
    "                               for s in batch.state])\n",
    "    tuple_next_state_batch = tuple([online_net_.obs_to_model_input(obs=s_)\n",
    "                                    for s_ in batch.next_state])\n",
    "    \n",
    "    # turn the batch elements (lists) into pytorch tensors\n",
    "    state_batch = torch.cat(tuple_state_batch).float().to(device)\n",
    "    next_state_batch = torch.cat(tuple_next_state_batch).float().to(device)\n",
    "    action_batch = torch.tensor(batch.action, device=device)\n",
    "    reward_batch = torch.tensor(batch.reward, device=device)\n",
    "    not_done_mask = 1 - torch.tensor(batch.done, device=device, dtype=torch.int)\n",
    "\n",
    "    # predict the Q-values for the (state,action) pairs in the batch\n",
    "    state_action_values = compute_q_vals(model_=online_net_, state_batch_=state_batch)\n",
    "    state_action_values = state_action_values.gather(1, action_batch.unsqueeze(1))\n",
    "\n",
    "    next_state_values = compute_next_q_vals(target_net_=online_net_, \n",
    "                                            next_state_batch_=next_state_batch)\n",
    "        \n",
    "    next_state_values = next_state_values.max(1)[0]\n",
    "\n",
    "    # nega-max target (MINIMAX DQN)\n",
    "    target = reward_batch - not_done_mask*gamma_*next_state_values\n",
    "\n",
    "    # Compute regression loss between predicted Q values and targets\n",
    "    loss = loss_func_(state_action_values, target.unsqueeze(1))\n",
    "\n",
    "    # Take an SGD step\n",
    "    optimizer_.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'losses': [],\n",
    "           'vs_random_win_rate': [], 'vs_random_avg_game_len': [],\n",
    "           'vs_1StepLA_win_rate': [], 'vs_1StepLA_avg_game_len': [],\n",
    "           'vs_old_self_win_rate': [], 'vs_old_self_avg_game_len': [],\n",
    "           'comp_every': hparams['comp_every'], 'comp_n_episodes': 100,\n",
    "          }\n",
    "\n",
    "vs_1StepLA_best_win_rate = 0.5\n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "\n",
    "env = ConnectGameEnv()\n",
    "comp_env = ConnectGameEnv()\n",
    "\n",
    "random_opponent = RandomAgent()\n",
    "oneStepLA = NStepLookaheadAgent(n=1, prefer_central_columns=True)\n",
    "\n",
    "episode_count = 0\n",
    "step_count = 0\n",
    "while step_count < hparams['num_steps']:\n",
    "    \n",
    "    # compute the exploration rate for the trainig episode\n",
    "    eps = compute_eps_threshold(step_count)\n",
    "\n",
    "    # generate a new training episode (self-play)\n",
    "    memory.push_self_play_episode_transitions(\n",
    "        agent=dqn_agent,\n",
    "        env=env,\n",
    "        init_random_obs=True,\n",
    "        push_symmetric=True,\n",
    "        exploration_rate=eps\n",
    "    )\n",
    "        \n",
    "    episode_count += 1\n",
    "    if len(memory) < hparams['min_memory_size']:\n",
    "        continue\n",
    "    \n",
    "    for _ in range(hparams['n_updates_per_new_episode']):\n",
    "        step_count += 1\n",
    "        # Perform one step of the optimization\n",
    "        loss = training_step(online_net_=dqn_agent.model, \n",
    "                             target_net_=target_net, \n",
    "                             optimizer_=optimizer,\n",
    "                             loss_func_=loss_func, \n",
    "                             memory_=memory, \n",
    "                             batch_size_=hparams['batch_size'], \n",
    "                             gamma_=hparams['gamma'])\n",
    "\n",
    "        history['losses'].append(loss)\n",
    "        \n",
    "        # update the target_network\n",
    "        if step_count % hparams['target_net_update_every'] == 0:\n",
    "            load_state_dict(from_=dqn_agent.model, to_=target_net)\n",
    "    \n",
    "        # display information about the training process\n",
    "        if step_count % hparams['loss_log_every'] == 0:\n",
    "            last_losses = history['losses'][-hparams['loss_log_every']:]\n",
    "            print(f\"Step: {step_count}/{hparams['num_steps']}    \" +\n",
    "                  f\"AvgLoss: {round(np.mean(last_losses),4)}\")\n",
    "        \n",
    "        # compete against the opponents to measure the performance\n",
    "        if step_count % hparams['comp_every'] == 0:\n",
    "            # compete against the Random Agent\n",
    "            dqn_agent.model.eval()\n",
    "            with torch.no_grad():\n",
    "                res1, o1 = competition(\n",
    "                    env=comp_env, \n",
    "                    agent1=dqn_agent, \n",
    "                    agent2=random_opponent,\n",
    "                    progress_bar=False)\n",
    "            win_rate_rand = round(res1['win_rate1'], 3)\n",
    "            print(f\"    {win_rate_rand} vs. RAND\" +\n",
    "                  f\"    avg_len={round(res1['avg_game_len'], 2)}\")\n",
    "            history['vs_random_win_rate'].append(win_rate_rand)\n",
    "            history['vs_random_avg_game_len'].append(res1['avg_game_len'])\n",
    "        \n",
    "            # compete againts the old (stable) version of the network\n",
    "            old_dqn_agent.model.eval()\n",
    "            with torch.no_grad():\n",
    "                res2, o2 = competition(\n",
    "                    env=comp_env, \n",
    "                    agent1=dqn_agent, \n",
    "                    agent2=old_dqn_agent,\n",
    "                    progress_bar=False,\n",
    "                )\n",
    "            win_rate_self = round(res2['win_rate1'], 3)\n",
    "            print(f\"    {win_rate_self} vs. SELF\" +\n",
    "                  f\"    avg_len={round(res2['avg_game_len'], 2)}\")\n",
    "            history['vs_old_self_win_rate'].append(win_rate_self)\n",
    "            history['vs_old_self_avg_game_len'].append(res2['avg_game_len'])\n",
    "        \n",
    "            # compete against the 1StepLA\n",
    "            with torch.no_grad():\n",
    "                res3, o3 = competition(\n",
    "                    env=comp_env, \n",
    "                    agent1=dqn_agent,\n",
    "                    agent2=oneStepLA,\n",
    "                    progress_bar=False,\n",
    "                )\n",
    "            win_rate_1StepLA = round(res3['win_rate1'], 3)\n",
    "            print(f\"    {win_rate_1StepLA} vs. 1StepLA\" +\n",
    "                  f\"    avg_len={round(res3['avg_game_len'], 2)}\")\n",
    "            history['vs_1StepLA_win_rate'].append(win_rate_1StepLA)\n",
    "            history['vs_1StepLA_avg_game_len'].append(res3['avg_game_len'])\n",
    "            \n",
    "            if win_rate_1StepLA > vs_1StepLA_best_win_rate:\n",
    "                vs_1StepLA_best_win_rate = win_rate_1StepLA\n",
    "                load_state_dict(from_=dqn_agent.model, to_=old_dqn_agent.model)\n",
    "                old_dqn_agent.model.eval()\n",
    "                load_state_dict(from_=dqn_agent.model, to_=target_net)\n",
    "                target_net.eval()\n",
    "                if SAVE_MODELS:\n",
    "                    file_name = f\"checkpoints/\" + save_best_vs_1StepLA_file.format(win_rate=int(win_rate_1StepLA*100))\n",
    "                    dqn_agent.model.save_weights(\n",
    "                        file_path=file_name,\n",
    "                        training_hparams=hparams,\n",
    "                    )\n",
    "                    print(f\"        new best {file_name} is saved!!!\")\n",
    "            elif win_rate_1StepLA <= vs_1StepLA_best_win_rate-hparams['vs_1StepLA_win_rate_decrease_to_undo_updates']:\n",
    "                load_state_dict(from_=old_dqn_agent.model, to_=dqn_agent.model)\n",
    "                load_state_dict(from_=old_dqn_agent.model, to_=target_net)\n",
    "                target_net.eval()\n",
    "                print(\"        undoing last updates...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training losses\n",
    "                       \n",
    "if hparams['is_dueling_dqn']:\n",
    "    name = 'Dueling DQN'\n",
    "else:\n",
    "    name = 'Vanilla DQN'\n",
    "\n",
    "data = moving_average(history['losses'][1000:100000], w=200)\n",
    "x_vals = [x/1000 for x in range(len(data))]\n",
    "plt.plot(x_vals, data)\n",
    "plt.title(f'{name} Training Loss (MSE)')\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"loss\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10))\n",
    "#plt.gca().yaxis.set_major_locator(MultipleLocator(0.025))\n",
    "plt.ylim(0.36,0.58)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = len(history['vs_old_self_win_rate']) * history['comp_every']\n",
    "x_vals = range(0, num_updates, history['comp_every'])\n",
    "x_vals = [x/1000 for x in x_vals]\n",
    "data = history['vs_old_self_avg_game_len']\n",
    "\n",
    "plt.title(f'{name} self-play game length')\n",
    "plt.plot(x_vals, data)\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"game length\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10))\n",
    "plt.axhline(42, linestyle='--', alpha=0.4)\n",
    "plt.axhline(7, linestyle='--', alpha=0.4)\n",
    "plt.ylim(-1, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = len(history['vs_1StepLA_win_rate']) * history['comp_every']\n",
    "x_vals = range(0, num_updates, history['comp_every'])\n",
    "x_vals = [x/1000 for x in x_vals]\n",
    "data = history['vs_1StepLA_win_rate']\n",
    "\n",
    "plt.plot(x_vals, data)\n",
    "plt.title(f'{name} win rate vs 1StepLA')\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"win rate\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10))\n",
    "plt.axhline(1, linestyle='--', alpha=0.4)\n",
    "plt.axhline(0.5, linestyle='--', alpha=0.4)\n",
    "plt.ylim(0.35, 1.09)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
