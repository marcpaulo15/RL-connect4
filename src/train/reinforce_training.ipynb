{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a REINFORCE (with baseline) Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOAL:** To train a _REINFORCE with baseline Agent_ to play Connect4. This is the second part of our training pipeline (the second part also includes other RL algorithms):\n",
    "  - **Part 1) Supervised Learning**\n",
    "    - refer to *'src/train/part1_supervised_learning.ipynb'*.\n",
    "    - RESULT: a pre-trained network with basic knowledge of the game\n",
    "  - **Part 2) Reinforcement Learning**\n",
    "    - In this case: REINFORCE with baseline\n",
    "    - **TRANFER LEARNING FROM PART 1:**\n",
    "      - Load the pre-trained weights from Part 1\n",
    "      - Freeze the convolutional block (*feature extractor*, it is not trained here)\n",
    "      - Train the rest of Fully Connected to estimate the optimal policy and the state values.\n",
    "<br>\n",
    "\n",
    "**METHOD:**\n",
    "   - We used an *Episode Buffer* to store an episode before updating\n",
    "       - exponent for reward backpropagation = 3\n",
    "       - for more details on the implementation refer to '*src/data/replay_memory.py*'\n",
    "   - The network architecture we used is defined in '*src/models/architectures/cnet128.json*'\n",
    "   - We applied *transfer learning* to use the knowledge learned in '*src/train/part1_supervised_learning.ipynb*'\n",
    "       - 1. load the network weights from '*src/models/saved_models/network_128.pt*'\n",
    "       - 2. freeze the convolutional block (*feature extractor*)\n",
    "       - 3. train the fully-connected layers to learn the policy and the state values\n",
    "   - There is an '*old agent*' that is an older and stable version of the agent. It is updated when:\n",
    "       - the agent achieves a new best win rate against the 1-StepLA Agent\n",
    "   - When the performance of the current network decreases significantly, the latest changes are undone and it goes back to the most recent *old weights*\n",
    "<br>\n",
    "\n",
    "**TRAINING:**\n",
    "   - We trained for 100k time steps\n",
    "   - The learning hyperparameters are:\n",
    "       - c1 = 0.75\n",
    "       - learning rate = 5e-6\n",
    "       - weight decay (L2 regularization) = 1e-3\n",
    "       - discount factor (gamma) = 0.95\n",
    "       - loss function (state value) = Smooth L1\n",
    "   - Every 1000 updates, the REINFORCE Agent competes against:\n",
    "       - vs the Random Agent\n",
    "       - vs the older network\n",
    "       - vs the 1-Step Lookahead Agent\n",
    "<br>\n",
    "\n",
    "**REINFORCE RESULTS:**\n",
    "   - Our best REINFORCE gent beats the 1-Step LookAhead Agent **â‰ˆ59%** of the time\n",
    "   - The weights of the model are saved in '*src/models/saved_models/best_reinforce.pt*'\n",
    "   - The training hyperaparameters are saved in '*src/models/saved_models/best_reinforce_hparams.json*'\n",
    "   - Plots of the training losses\n",
    "   - Plots of the average game length in self-play games\n",
    "   - Plots of the evolution of the win rate vs 1StepLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR PATH HERE\n",
    "code_dir = '/home/marc/Escritorio/RL-connect4/'\n",
    "\n",
    "if os.path.isdir(code_dir):\n",
    "    # local environment\n",
    "    os.chdir(code_dir)\n",
    "    print(f\"directory -> '{code_dir }'\")\n",
    "else:\n",
    "    # google colab environment\n",
    "    if os.path.isdir('./src'):\n",
    "        print(\"'./src' dir already exists\")\n",
    "    else:  # not unzipped yet\n",
    "        !unzip -q src.zip\n",
    "        print(\"'./src.zip' file successfully unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.baselines.random_agent import RandomAgent\n",
    "from src.agents.baselines.n_step_lookahead_agent import NStepLookaheadAgent\n",
    "from src.models.custom_network import CustomNetwork\n",
    "from src.agents.trainable.pg_agent import PGAgent\n",
    "from src.environment.connect_game_env import ConnectGameEnv\n",
    "from src.data.replay_memory import ReplayMemory\n",
    "from src.eval.competition import competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%d%m_%H%M\")\n",
    "print(f\"'{timestamp}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODELS = False  # if False, it is debug mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # environment, data, memory\n",
    "    'reward_backprop_exponent': 3,\n",
    "\n",
    "    # agent properties and model architecture\n",
    "    'avg_symmetric_probs': True,\n",
    "    'model_arch_path': './src/models/architectures/cnet128.json',\n",
    "    'load_weights_path': './src/models/saved_models/supervised_cnet128.pt',\n",
    "    'freeze_conv_block': True,\n",
    "\n",
    "    # Information displayed while training\n",
    "    'loss_log_every': 2,\n",
    "    'comp_every': 10,\n",
    "    'vs_1StepLA_win_rate_decrease_to_undo_updates': 0.08,\n",
    "    'moving_avg': 100,\n",
    "    \n",
    "    # Loss\n",
    "    'c1': 0.75,\n",
    "\n",
    "    # Training loop params\n",
    "    'num_episodes': 10000,\n",
    "    'gamma' : 0.95,\n",
    "    'weight_decay': 1e-3,\n",
    "    'lr': 1e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) REINFORCE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(from_: nn.Module, to_: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Copies the weights from the module 'from_' to the module 'to_'\n",
    "    It ensures that the convolutional block is frozen (if necessary)\n",
    "    \"\"\"\n",
    "\n",
    "    to_.load_state_dict(from_.state_dict())\n",
    "    if hparams['freeze_conv_block']:\n",
    "        for param in to_.conv_block.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "def create_new_policy_model() -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create a Policy Network following the architecture in 'model_arch_file',\n",
    "    and nitializing the weights as 'load_weights_path'.\n",
    "    If 'freeze_conv_block', set the convolutional gradients to zero\n",
    "    \"\"\"\n",
    "\n",
    "    policy_net = CustomNetwork.from_architecture(\n",
    "        file_path=hparams['model_arch_path']\n",
    "    ).to(device)\n",
    "\n",
    "    policy_net.load_weights(hparams['load_weights_path'])\n",
    "\n",
    "    for param in policy_net.conv_block.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return policy_net\n",
    "\n",
    "\n",
    "def create_new_reinforce_agent():\n",
    "    \"\"\"\n",
    "    Create a new REINFORCE Agent\n",
    "    \"\"\"\n",
    "\n",
    "    model_ = create_new_policy_model()\n",
    "    agent_ = PGAgent(model=model_,\n",
    "                      avg_symmetric_probs=hparams['avg_symmetric_probs'],\n",
    "                      name=\"Reinforce Agent\")\n",
    "    return agent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_agent = create_new_reinforce_agent()\n",
    "\n",
    "print(\"REINFORCE Agent device is cuda?\", next(reinforce_agent.model.parameters()).is_cuda)\n",
    "print()\n",
    "print(summary(reinforce_agent.model, input_size=reinforce_agent.model.input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_reinforce_agent = create_new_reinforce_agent()\n",
    "load_state_dict(from_=reinforce_agent.model, to_=old_reinforce_agent.model)\n",
    "old_reinforce_agent.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = reinforce_agent.name.replace(' ', '_')\n",
    "save_best_vs_1StepLA_file = f'{agent_name}_'+'{win_rate}_vs_1StepLA_'+f'{timestamp}.pt'\n",
    "\n",
    "print('\"' + save_best_vs_1StepLA_file + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Episode Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayMemory(\n",
    "    capacity=200,\n",
    "    reward_backprop_exponent=hparams['reward_backprop_exponent']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Prepare the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=reinforce_agent.model.parameters(),\n",
    "    lr=hparams['lr'],\n",
    "    weight_decay=hparams['weight_decay']\n",
    ")\n",
    "\n",
    "comp_env = ConnectGameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(policy_net_, buffer_, optimizer_, gamma_):\n",
    "\n",
    "    policy_net_.train()\n",
    "    \n",
    "    # separate the symmetric episodes\n",
    "    ep_experience = buffer_.all_data()\n",
    "    episodes_list = [[]]\n",
    "    for t in ep_experience:\n",
    "        episodes_list[-1].append(t)\n",
    "        if t.done:\n",
    "            episodes_list.append([])\n",
    "    episodes_list.pop()\n",
    "\n",
    "    policy_losses, value_losses, entropies = [], [], []\n",
    "    for ep_transitions_ in episodes_list:\n",
    "\n",
    "        if len(ep_transitions_) == 0:\n",
    "            continue\n",
    "\n",
    "        ep_transitions = copy.deepcopy(ep_transitions_)\n",
    "        batch = buffer_.Transition(*zip(*ep_transitions))\n",
    "\n",
    "        state_batch_ = tuple([policy_net_.obs_to_model_input(obs=s)\n",
    "                              for s in batch.state])\n",
    "        state_batch = torch.cat(state_batch_).float().to(device)\n",
    "        action_batch = torch.tensor(batch.action, device=device)\n",
    "\n",
    "        returns = []\n",
    "        G = 0\n",
    "\n",
    "        # Compute the returns by reading the rewards vector backwards\n",
    "        for i, r in enumerate(list(batch.reward)[::-1]):\n",
    "            G = r - gamma_*G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float, device=device)\n",
    "\n",
    "        ep_policy_losses = []\n",
    "        ep_value_losses = []  \n",
    "        ep_entropies = []\n",
    "\n",
    "        i = 0\n",
    "        for s, G in zip(state_batch, returns):\n",
    "            logits, baseline = policy_net_(s.unsqueeze(0))\n",
    "            m = torch.distributions.Categorical(logits=logits)\n",
    "            log_prob = m.log_prob(action_batch[i])\n",
    "            entropy = m.entropy()\n",
    "            \n",
    "            advantage = G - baseline\n",
    "\n",
    "            policy_loss = - log_prob * advantage\n",
    "            \n",
    "            value_loss = nn.functional.smooth_l1_loss(baseline.squeeze(), G)\n",
    "            total_loss = policy_loss + hparams['c1']*value_loss\n",
    "            \n",
    "            optimizer_.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer_.step()\n",
    "            \n",
    "            ep_policy_losses.append(-policy_loss.item())\n",
    "            ep_value_losses.append(value_loss.item())\n",
    "            ep_entropies.append(entropy.item())\n",
    "            i += 1\n",
    "\n",
    "        policy_losses.extend(ep_policy_losses)\n",
    "        value_losses.extend(ep_value_losses)\n",
    "        entropies.extend(ep_entropies)\n",
    "\n",
    "    return policy_losses, value_losses, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'n_updates': 0, 'policy_losses': [], 'value_losses': [], 'entropies': [], \n",
    "           'vs_random_win_rate': [], 'vs_random_avg_game_len': [],\n",
    "           'vs_1StepLA_win_rate': [], 'vs_1StepLA_avg_game_len': [],\n",
    "           'vs_old_self_win_rate': [], 'vs_old_self_avg_game_len': [],\n",
    "           'comp_every': hparams['comp_every'], 'comp_n_episodes': 100,\n",
    "          }\n",
    "\n",
    "vs_1StepLA_best_win_rate = 0.50\n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "\n",
    "env = ConnectGameEnv()\n",
    "comp_env = ConnectGameEnv()\n",
    "\n",
    "random_opponent = RandomAgent()\n",
    "oneStepLA = NStepLookaheadAgent(n=1, prefer_central_columns=True)\n",
    "\n",
    "episode_count = 0\n",
    "for i_episode in range(hparams['num_episodes']):\n",
    "\n",
    "    buffer.reset()\n",
    "\n",
    "    buffer.push_self_play_episode_transitions(\n",
    "        agent=reinforce_agent,\n",
    "        env=env,\n",
    "        init_random_obs=True,\n",
    "        push_symmetric=True\n",
    "    )\n",
    "\n",
    "    # Perform one step of the optimization\n",
    "    policy_losses, value_losses, entropies = training_step(\n",
    "        policy_net_=reinforce_agent.model,\n",
    "        buffer_=buffer,\n",
    "        optimizer_=optimizer,\n",
    "        gamma_=hparams['gamma']\n",
    "    )\n",
    "    \n",
    "    history['n_updates'] += len(policy_losses)\n",
    "    history['policy_losses'].extend(policy_losses)\n",
    "    history['value_losses'].extend(value_losses)\n",
    "    history['entropies'].extend(entropies)\n",
    "\n",
    "    # display information about the training process\n",
    "    if (i_episode+1) % hparams['loss_log_every'] == 0:\n",
    "        policy_losses_ = history['policy_losses'][-hparams['moving_avg']:]\n",
    "        value_losses_ = history['value_losses'][-hparams['moving_avg']:]\n",
    "        entropies_ = history['entropies'][-hparams['moving_avg']:]\n",
    "        print(f\"Episode: {i_episode+1}/{hparams['num_episodes']}   \" +\n",
    "              f\"Nupdates: {history['n_updates']}   \"\n",
    "              f\"PolLoss: {round(np.mean(policy_losses_), 3)}   \" +\n",
    "              f\"ValLoss: {round(np.mean(value_losses_), 3)}   \" +\n",
    "              f\"Entropy: {round(np.mean(entropies_), 3)}\"\n",
    "              )\n",
    "\n",
    "    # compete against the Random Agent\n",
    "    if (i_episode+1) % hparams['comp_every'] == 0:\n",
    "        reinforce_agent.model.eval()\n",
    "        with torch.no_grad():\n",
    "            res1, o1 = competition(\n",
    "                env=comp_env,\n",
    "                agent1=reinforce_agent,\n",
    "                agent2=random_opponent,\n",
    "                progress_bar=False\n",
    "            )\n",
    "        win_rate_rand = round(res1['win_rate1'], 3)\n",
    "        print(f\"    {win_rate_rand} vs. RAND\" +\n",
    "              f\"    avg_len={round(res1['avg_game_len'], 2)}\")\n",
    "        history['vs_random_win_rate'].append(win_rate_rand)\n",
    "        history['vs_random_avg_game_len'].append(res1['avg_game_len'])\n",
    "\n",
    "        # compete againts the old (stable) version of the network\n",
    "    \n",
    "        reinforce_agent.model.eval()\n",
    "        old_reinforce_agent.model.eval()\n",
    "        with torch.no_grad():\n",
    "            res2, o2 = competition(\n",
    "                env=comp_env,\n",
    "                agent1=reinforce_agent,\n",
    "                agent2=old_reinforce_agent,\n",
    "                progress_bar=False\n",
    "            )\n",
    "        win_rate_self = round(res2['win_rate1'], 3)\n",
    "        print(f\"    {win_rate_self} vs. SELF\" +\n",
    "              f\"    avg_len={round(res2['avg_game_len'], 2)}\")\n",
    "        history['vs_old_self_win_rate'].append(win_rate_self)\n",
    "        history['vs_old_self_avg_game_len'].append(res2['avg_game_len'])\n",
    "\n",
    "\n",
    "        # compete against the 1StepLA\n",
    "        reinforce_agent.model.eval()\n",
    "        with torch.no_grad():\n",
    "            res3, o3 = competition(\n",
    "                env=comp_env,\n",
    "                agent1=reinforce_agent,\n",
    "                agent2=oneStepLA,\n",
    "                progress_bar=False,\n",
    "            )\n",
    "        win_rate_1StepLA = round(res3['win_rate1'], 3)\n",
    "        print(f\"    {win_rate_1StepLA} vs. 1StepLA\" +\n",
    "              f\"    avg_len={round(res3['avg_game_len'], 2)}\")\n",
    "        history['vs_1StepLA_win_rate'].append(win_rate_1StepLA)\n",
    "        history['vs_1StepLA_avg_game_len'].append(res3['avg_game_len'])\n",
    "        if win_rate_1StepLA > vs_1StepLA_best_win_rate:\n",
    "            vs_1StepLA_best_win_rate = win_rate_1StepLA\n",
    "            load_state_dict(from_=reinforce_agent.model, to_=old_reinforce_agent.model)\n",
    "            old_reinforce_agent.model.eval()\n",
    "            if SAVE_MODELS:\n",
    "                file_name = (\n",
    "                    f\"checkpoints/\" + save_best_vs_1StepLA_file.format(win_rate=int(win_rate_1StepLA*100))\n",
    "                )\n",
    "                reinforce_agent.model.save_weights(\n",
    "                    file_path=file_name,\n",
    "                    training_hparams=hparams,\n",
    "                )\n",
    "                print(f\"        new best {file_name} is saved!!!\")\n",
    "        elif win_rate_1StepLA <= vs_1StepLA_best_win_rate-hparams['vs_1StepLA_win_rate_decrease_to_undo_updates']:\n",
    "            load_state_dict(from_=old_reinforce_agent.model, to_=reinforce_agent.model)\n",
    "            print(\"        undoing last updates...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_losses = np.array(history['policy_losses'])\n",
    "v_losses = np.array(history['value_losses'])\n",
    "total_losses = p_losses + hparams['c1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = moving_average(total_losses[:100000], w=1000)\n",
    "x_vals = [x/1000 for x in range(len(data))]\n",
    "\n",
    "plt.plot(x_vals, data)\n",
    "plt.title('REINFORCE with baseline Training Loss')\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"loss\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10))\n",
    "#plt.gca().yaxis.set_major_locator(MultipleLocator(0.025))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_updates = 100000\n",
    "#x_vals = range(0, num_updates, num_updates//len(history['vs_1StepLA_win_rate']))\n",
    "#x_vals = [x/1000 for x in x_vals]\n",
    "data = history['vs_1StepLA_win_rate']\n",
    "x_vals = [x/1000 for x in range(len(data))]\n",
    "\n",
    "plt.plot(x_vals, data)\n",
    "plt.title('REINFORCE with baseline Win rate vs 1StepLA')\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"win rate\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10000))\n",
    "plt.axhline(1, linestyle='--', alpha=0.4)\n",
    "plt.axhline(0.5, linestyle='--', alpha=0.4)\n",
    "plt.ylim(0.35, 1.09)\n",
    "#plt.xlim(0, 105)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_updates = 100000\n",
    "#x_vals = range(0, num_updates, num_updates//len(history['vs_1StepLA_win_rate']))\n",
    "#x_vals = [x/1000 for x in x_vals]\n",
    "data = history['vs_old_self_avg_game_len']\n",
    "x_vals = [x/1000 for x in range(len(data))]\n",
    "\n",
    "plt.title('REINFORCE with baseline Self-play game length')\n",
    "plt.plot(x_vals, data)\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"game length\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10))\n",
    "plt.axhline(42, linestyle='--', alpha=0.4)\n",
    "plt.axhline(7, linestyle='--', alpha=0.4)\n",
    "plt.ylim(-1, 45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
