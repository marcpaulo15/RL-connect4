{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a PPO Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOAL:** To train a _PPO Agent_ to play Connect4. This is the second part of our training pipeline (the second part also includes other RL algorithms):\n",
    "  - **Part 1) Supervised Learning**\n",
    "    - refer to *'src/train/part1_supervised_learning.ipynb'*.\n",
    "    - RESULT: a pre-trained network with basic knowledge of the game\n",
    "  - **Part 2) Reinforcement Learning**\n",
    "    - In this case: Proximal Policy Optimization\n",
    "    - **TRANFER LEARNING FROM PART 1:**\n",
    "      - Load the pre-trained weights from Part 1\n",
    "      - Freeze the convolutional block (*feature extractor*, it is not trained here)\n",
    "      - Train the rest of Fully Connected to estimate the optimal policy and the state values.\n",
    "<br>\n",
    "\n",
    "**METHOD:**\n",
    "   - We used an *Experience Buffer* to store different episodes\n",
    "       - capacity = 2000\n",
    "       - exponent for reward backpropagation = 3\n",
    "       - for more details on the implementation refer to '*src/data/replay_memory.py*'\n",
    "   - The network architecture we used is defined in '*src/models/architectures/cnet128.json*'\n",
    "   - We applied *transfer learning* to use the knowledge learned in '*src/train/part1_supervised_learning.ipynb*'\n",
    "       - 1. load the network weights from '*src/models/saved_models/network_128.pt*'\n",
    "       - 2. freeze the convolutional block (*feature extractor*)\n",
    "       - 3. train the fully-connected layers to learn the policy and the state values\n",
    "   - There is an '*old agent*' that is an older and stable version of the agent. It is updated when:\n",
    "       - the agent achieve a new best win rate against the 1-StepLA Agent\n",
    "   - When the performance of the current network decreases significantly, the latest changes are undone and it goes back to the most recent *old weights*\n",
    "<br>\n",
    "\n",
    "**TRAINING:**\n",
    "   - We trained for 100k time steps (aprox 320 iterations)\n",
    "   - The learning hyperparameters are:\n",
    "       - buffer capacity = 2000\n",
    "       - ppo epochs = 5\n",
    "       - c1 = 0.75\n",
    "       - c2 = 0.04\n",
    "       - learning rate = 1e-4\n",
    "       - batch size = 32\n",
    "       - weight decay (L2 regularization) = 5e-5\n",
    "       - discount factor (gamma) = 0.95\n",
    "       - loss function (critic) = Smooth L1\n",
    "   - Every 1000 updates, the PPO agent competes against:\n",
    "       - vs the Random Agent\n",
    "       - vs the older network\n",
    "       - vs the 1-Step Lookahead Agent\n",
    "<br>\n",
    "\n",
    "**PPO RESULTS:**\n",
    "   - Our best PPO gent beats the 1-Step LookAhead Agent **â‰ˆ84%** of the time\n",
    "   - The weights of the model are saved in '*src/models/saved_models/best_ppo.pt*'\n",
    "   - The training hyperaparameters are saved in '*src/models/saved_models/best_ppo_hparams.json*'\n",
    "   - Plots of the training losses\n",
    "   - Plots of the average game length in self-play games\n",
    "   - Plots of the evolution of the win rate vs 1StepLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import BatchSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR PATH HERE\n",
    "code_dir = '/home/marc/Escritorio/RL-connect4/'\n",
    "\n",
    "if os.path.isdir(code_dir):\n",
    "    # local environment\n",
    "    os.chdir(code_dir)\n",
    "    print(f\"directory -> '{code_dir }'\")\n",
    "else:\n",
    "    # google colab environment\n",
    "    if os.path.isdir('./src'):\n",
    "        print(\"'./src' dir already exists\")\n",
    "    else:  # not unzipped yet\n",
    "        !unzip -q src.zip\n",
    "        print(\"'./src.zip' file successfully unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.baselines.random_agent import RandomAgent\n",
    "from src.agents.baselines.n_step_lookahead_agent import NStepLookaheadAgent\n",
    "from src.models.custom_network import CustomNetwork\n",
    "from src.agents.trainable.pg_agent import PGAgent\n",
    "from src.environment.connect_game_env import ConnectGameEnv\n",
    "from src.data.replay_memory import ReplayMemory\n",
    "from src.eval.competition import competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp =datetime.now().strftime(\"%d%m_%H%M\")\n",
    "print(f\"'{timestamp}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODELS = False  # if False, it is debug mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # environment, data, memory\n",
    "    'buffer_capacity': 2000,\n",
    "    'reward_backprop_exponent': 3,\n",
    "    \n",
    "    # agent properties and model architecture\n",
    "    'avg_symmetric_probs': True,\n",
    "    'model_arch_path': './src/models/architectures/cnet128.json',\n",
    "    'pretrained_model_weights': './src/models/saved_models/supervised_cnet128.pt',\n",
    "    'freeze_conv_block': True,\n",
    "    \n",
    "    # Information displayed while training\n",
    "    'loss_log_every': 1,\n",
    "    'comp_every': 5,\n",
    "    'vs_1StepLA_win_rate_decrease_to_undo_updates': 0.08,\n",
    "    'moving_avg': 100,\n",
    "        \n",
    "    # PPO hyperparameters\n",
    "    'num_iterations': 320,\n",
    "    'clip_param': 0.2,\n",
    "    'ppo_epoch': 5,\n",
    "    'c1': 0.75,\n",
    "    'c2': 0.04,\n",
    "    \n",
    "    # Training loop params\n",
    "    'batch_size': 32,\n",
    "    'gamma' : 0.95,\n",
    "    'weight_decay': 5e-5,\n",
    "    'lr': 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(from_: nn.Module, to_: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Copies the weights from the module 'from_' to the module 'to_'\n",
    "    Loading the state_dict from a module does not keep the\n",
    "    convolutional_block frozen, so it has to be done manually.\n",
    "    \"\"\"\n",
    "\n",
    "    to_.load_state_dict(from_.state_dict())\n",
    "    if hparams['freeze_conv_block']:\n",
    "        for param in to_.conv_block.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "            \n",
    "\n",
    "def create_model() -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create a PPO network following the architecture in 'model_arch_file',\n",
    "    and initializing the network with the weights stored in 'load_weights_path'. \n",
    "    \"\"\"\n",
    "\n",
    "    ppo_net = CustomNetwork.from_architecture(\n",
    "        file_path=hparams['model_arch_path']\n",
    "    ).to(device)\n",
    "    \n",
    "    ppo_net.load_weights(hparams['pretrained_model_weights'])\n",
    "    \n",
    "    for param in ppo_net.conv_block.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return ppo_net\n",
    "\n",
    "\n",
    "def create_agent():\n",
    "    \"\"\"\n",
    "    Create a new PPO Agent\n",
    "    \"\"\"\n",
    "\n",
    "    model_ = create_model()\n",
    "    agent_ = PGAgent(\n",
    "        model=model_,\n",
    "        stochastic_mode=True,\n",
    "        avg_symmetric_probs=hparams['avg_symmetric_probs'],\n",
    "        name='PPO Agent'\n",
    "    )\n",
    "    return agent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = create_agent()\n",
    "\n",
    "print(\"model device is cuda?\", next(ppo_agent.model.parameters()).is_cuda)\n",
    "print()\n",
    "print(summary(ppo_agent.model, input_size=ppo_agent.model.input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ppo_agent = create_agent()\n",
    "load_state_dict(from_=ppo_agent.model, to_=old_ppo_agent.model)\n",
    "old_ppo_agent.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = ppo_agent.name.replace(' ', '_')\n",
    "save_best_vs_1StepLA_file = f'{agent_name}_'+'{win_rate}_vs_1StepLA_'+f'{timestamp}.pt'\n",
    "\n",
    "print('\"' + save_best_vs_1StepLA_file + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Experience Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayMemory(\n",
    "    capacity=hparams['buffer_capacity'],\n",
    "    reward_backprop_exponent=hparams['reward_backprop_exponent']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Prepare the training lopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=ppo_agent.model.parameters(), \n",
    "    lr=hparams['lr'],\n",
    "    weight_decay=hparams['weight_decay']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_target_value(policy_: nn.Module, states_: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Compute the value of the given states.\n",
    "    NOTE: output does not contain gradient, it is part of the target\n",
    "    It takes advantage of the board symmetry.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, v = policy_(states_)\n",
    "\n",
    "    sym_states = torch.flip(states_, dims=[-1])\n",
    "    with torch.no_grad():\n",
    "        _, sym_v = policy_(sym_states)\n",
    "    \n",
    "    avg_v = (v + sym_v) / 2\n",
    "    return avg_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(policy_, \n",
    "                  optimizer_,\n",
    "                  buffer_, \n",
    "                  hparams_, \n",
    "                  critic_loss_func_):\n",
    "\n",
    "    gamma = hparams_['gamma']\n",
    "    ppo_epoch = hparams_['ppo_epoch']\n",
    "    batch_size = hparams_['batch_size']\n",
    "    clip_param = hparams_['clip_param']\n",
    "    c1 = hparams_['c1']\n",
    "    c2 = hparams_['c2']  # entropy coefficient\n",
    "    \n",
    "    policy_.train()\n",
    "    \n",
    "    data = copy.deepcopy(buffer_.all_data())\n",
    "    transitions = buffer_.Transition(*zip(*data))\n",
    "\n",
    "    tuple_state_batch = tuple([policy_.obs_to_model_input(obs=s)\n",
    "                               for s in transitions.state])\n",
    "    tuple_next_state_batch = tuple([policy_.obs_to_model_input(obs=s_)\n",
    "                                    for s_ in transitions.next_state])\n",
    "    \n",
    "    state_batch = torch.cat(tuple_state_batch).float().to(device)\n",
    "    next_state_batch = torch.cat(tuple_next_state_batch).float().to(device)\n",
    "    action_batch = torch.tensor(transitions.action, dtype=torch.long, device=device)\n",
    "    reward_batch = torch.tensor(transitions.reward, dtype=torch.float, device=device).view(-1,1)\n",
    "    not_done_mask = 1 - torch.tensor(transitions.done, device=device, dtype=torch.int).view(-1,1)\n",
    "\n",
    "    \n",
    "    old_a_logp = torch.tensor(transitions.log_prob, \n",
    "                              dtype=torch.float,\n",
    "                              device=device).view(-1, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_v = reward_batch - gamma*not_done_mask*compute_avg_target_value(policy_, next_state_batch)\n",
    "        adv = target_v - compute_avg_target_value(policy_, state_batch)\n",
    "        \n",
    "    total_losses, policy_losses, value_losses, entropies, ratios = [], [], [], [], []\n",
    "\n",
    "    for epoch in range(ppo_epoch):\n",
    "        for index in BatchSampler(SubsetRandomSampler(range(buffer_.capacity)), batch_size, False):\n",
    "            logits, _ = policy_(state_batch[index])\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            entropy = dist.entropy()\n",
    "            \n",
    "            a_logp = dist.log_prob(action_batch[index]).unsqueeze(-1)\n",
    "            a_logp_old = old_a_logp[index]\n",
    "\n",
    "            # Compute ratio: pi/pi_old = e^(ln pi - ln pi_old)\n",
    "            ratio = torch.exp(a_logp - a_logp_old)\n",
    "\n",
    "            surr1 = ratio * adv[index]\n",
    "\n",
    "            surr2 = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param) * adv[index]\n",
    "\n",
    "            policy_loss = torch.min(surr1, surr2).mean()\n",
    "            value_loss = critic_loss_func_(policy_(state_batch[index])[1], target_v[index])\n",
    "            entropy = entropy.mean()\n",
    "\n",
    "            loss = - policy_loss + c1*value_loss - c2*entropy\n",
    "\n",
    "            optimizer_.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_.step()\n",
    "            \n",
    "            total_losses.append(loss.item())\n",
    "            policy_losses.append(-policy_loss.item())\n",
    "            value_losses.append(value_loss.item())\n",
    "            entropies.append(entropy.item())\n",
    "            ratios.append(ratio.mean().item())\n",
    "\n",
    "    return total_losses, policy_losses, value_losses, entropies, ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'total_losses': [], 'policy_losses': [], 'value_losses': [], \n",
    "           'entropies': [], 'ratios': [],\n",
    "           'vs_random_win_rate': [], 'vs_random_avg_game_len': [],\n",
    "           'vs_1StepLA_win_rate': [], 'vs_1StepLA_avg_game_len': [],\n",
    "           'vs_old_self_win_rate': [], 'vs_old_self_avg_game_len': [],\n",
    "           'comp_every': hparams['comp_every'], 'comp_n_episodes': 100,\n",
    "          }\n",
    "\n",
    "vs_1StepLA_best_win_rate = 0.5\n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "\n",
    "env = ConnectGameEnv()\n",
    "comp_env = ConnectGameEnv()\n",
    "\n",
    "random_opponent = RandomAgent()\n",
    "oneStepLA = NStepLookaheadAgent(n=1, prefer_central_columns=True)\n",
    "\n",
    "eps_ = np.finfo(np.float32).eps.item()\n",
    "n_updates = 0\n",
    "for i_iter in range(hparams['num_iterations']):\n",
    "    \n",
    "    buffer.reset()\n",
    "    ppo_agent.model.train()\n",
    "    \n",
    "    while len(buffer) != buffer.capacity:\n",
    "        buffer.push_self_play_episode_transitions(\n",
    "            agent=ppo_agent,\n",
    "            env=env,\n",
    "            init_random_obs=True,\n",
    "            push_symmetric=True,\n",
    "            exploration_rate=0.10,\n",
    "        )\n",
    "    \n",
    "    total_losses, policy_loss, value_loss, avg_entropy, ratios = training_step(\n",
    "        policy_=ppo_agent.model, \n",
    "        optimizer_=optimizer, \n",
    "        buffer_=buffer, \n",
    "        hparams_=hparams,\n",
    "        critic_loss_func_=critic_loss_func,\n",
    "    )\n",
    "    n_updates += len(total_losses)\n",
    "\n",
    "    history['total_losses'].extend(total_losses)\n",
    "    history['policy_losses'].extend(policy_loss)\n",
    "    history['value_losses'].extend(value_loss)\n",
    "    history['entropies'].extend(avg_entropy)\n",
    "    history['ratios'].extend(ratios)\n",
    "    \n",
    "    if (i_iter+1) % hparams['loss_log_every'] == 0:\n",
    "        last_total_loss_vals = history['total_losses'][-hparams['moving_avg']:]\n",
    "        last_policy_loss_vals = history['policy_losses'][-hparams['moving_avg']:]\n",
    "        last_value_loss_vals = history['value_losses'][-hparams['moving_avg']:]\n",
    "        last_avg_entropy_vals = history['entropies'][-hparams['moving_avg']:]\n",
    "        last_ratio_vals = history['ratios'][-hparams['moving_avg']:]\n",
    "        print(f\"Iter: {i_iter+1}/{hparams['num_iterations']}   \" +\n",
    "              f\"update: {n_updates}   \"\n",
    "              f\"Loss: {round(np.mean(last_total_loss_vals), 4)}   \" +\n",
    "              f\"P_Loss: {round(np.mean(last_policy_loss_vals), 4)}   \" +\n",
    "              f\"V_Loss: {round(np.mean(last_value_loss_vals), 4)}   \" +\n",
    "              f\"S: {round(np.mean(last_avg_entropy_vals), 3)}   \")\n",
    "    \n",
    "    # compete against the opponents to measure the performance\n",
    "    if (i_iter+1) % hparams['comp_every'] == 0:\n",
    "        # compete against the Random Agent\n",
    "        ppo_agent.model.eval()\n",
    "        with torch.no_grad():\n",
    "            res1, o1 = competition(\n",
    "                env=comp_env, \n",
    "                agent1=ppo_agent, \n",
    "                agent2=random_opponent,\n",
    "                progress_bar=False,\n",
    "            )\n",
    "        win_rate_rand = round(res1['win_rate1'], 3)\n",
    "        print(f\"    {win_rate_rand} vs. RAND\" +\n",
    "              f\"    avg_len={round(res1['avg_game_len'], 2)}\")\n",
    "        history['vs_random_win_rate'].append(win_rate_rand)\n",
    "        history['vs_random_avg_game_len'].append(res1['avg_game_len'])\n",
    "\n",
    "        # compete againts the old (stable) version of the network\n",
    "        old_ppo_agent.model.eval()\n",
    "        with torch.no_grad():\n",
    "            res2, o2 = competition(\n",
    "                env=comp_env, \n",
    "                agent1=ppo_agent, \n",
    "                agent2=old_ppo_agent,\n",
    "                progress_bar=False,\n",
    "            )\n",
    "        win_rate_self = round(res2['win_rate1'], 3)\n",
    "        print(f\"    {win_rate_self} vs. SELF\" +\n",
    "              f\"    avg_len={round(res2['avg_game_len'], 2)}\")\n",
    "        history['vs_old_self_win_rate'].append(win_rate_self)\n",
    "        history['vs_old_self_avg_game_len'].append(res2['avg_game_len'])\n",
    "\n",
    "        # compete against the Professional Player\n",
    "        with torch.no_grad():\n",
    "            res3, o3 = competition(\n",
    "                env=comp_env, \n",
    "                agent1=ppo_agent,\n",
    "                agent2=oneStepLA,\n",
    "                progress_bar=False,\n",
    "            )\n",
    "        win_rate_1StepLA = round(res3['win_rate1'], 3)\n",
    "        print(f\"    {win_rate_1StepLA} vs. 1StepLA\" +\n",
    "              f\"    avg_len={round(res3['avg_game_len'], 2)}\")\n",
    "        history['vs_1StepLA_win_rate'].append(win_rate_1StepLA)\n",
    "        history['vs_1StepLA_avg_game_len'].append(res3['avg_game_len'])\n",
    "\n",
    "        if win_rate_1StepLA > vs_1StepLA_best_win_rate:\n",
    "            vs_1StepLA_best_win_rate = win_rate_1StepLA\n",
    "            load_state_dict(from_=ppo_agent.model, to_=old_ppo_agent.model)\n",
    "            old_ppo_agent.model.eval()\n",
    "            if SAVE_MODELS:\n",
    "                file_name = (\n",
    "                    f\"checkpoints/\" + save_best_vs_1StepLA_file.format(win_rate=int(win_rate_1StepLA*100))\n",
    "                )\n",
    "                ppo_agent.model.save_weights(\n",
    "                    file_path=file_name,\n",
    "                    training_hparams=hparams,\n",
    "                )\n",
    "                print(f\"        new best {file_name} is saved!!!\")\n",
    "        elif win_rate_1StepLA <= vs_1StepLA_best_win_rate-hparams['vs_1StepLA_win_rate_decrease_to_undo_updates']:\n",
    "            load_state_dict(from_=old_ppo_agent.model, to_=ppo_agent.model)\n",
    "            print(\"        undoing last updates...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = moving_average(total_losses[:100000], w=1000)\n",
    "x_vals = [x/1000 for x in range(len(data))]\n",
    "\n",
    "plt.plot(x_vals, data)\n",
    "plt.title('PPO Training Loss')\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"loss\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10))\n",
    "#plt.gca().yaxis.set_major_locator(MultipleLocator(0.025))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_updates = 100000\n",
    "#x_vals = range(0, num_updates, num_updates//len(history['vs_1StepLA_win_rate']))\n",
    "#x_vals = [x/1000 for x in x_vals]\n",
    "data = history['vs_1StepLA_win_rate']\n",
    "x_vals = [x/1000 for x in range(len(data))]\n",
    "\n",
    "plt.plot(x_vals, data)\n",
    "plt.title('PPO win rate vs 1StepLA')\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"win rate\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10000))\n",
    "plt.axhline(1, linestyle='--', alpha=0.4)\n",
    "plt.axhline(0.5, linestyle='--', alpha=0.4)\n",
    "plt.ylim(0.35, 1.09)\n",
    "#plt.xlim(0, 105)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_updates = 100000\n",
    "#x_vals = range(0, num_updates, num_updates//len(history['vs_1StepLA_win_rate']))\n",
    "#x_vals = [x/1000 for x in x_vals]\n",
    "data = history['vs_old_self_avg_game_len']\n",
    "x_vals = [x/1000 for x in range(len(data))]\n",
    "\n",
    "plt.title('PPO self-play game length')\n",
    "plt.plot(x_vals, data)\n",
    "plt.xlabel(\"updates (in thousands)\")\n",
    "plt.ylabel(\"game length\")\n",
    "#plt.gca().xaxis.set_major_locator(MultipleLocator(10))\n",
    "plt.axhline(42, linestyle='--', alpha=0.4)\n",
    "plt.axhline(7, linestyle='--', alpha=0.4)\n",
    "plt.ylim(-1, 45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
