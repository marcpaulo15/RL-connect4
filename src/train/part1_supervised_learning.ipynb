{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1) Supervised Learning Task. Training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GOAL**: train a Neural Network to predict the actions of a mid-level player (classification task)\n",
    "<br>\n",
    "- **ABOUT THE TRAINING DATA:**\n",
    "    - our mid-level player is a _1StepLookaheadAgent_ instance\n",
    "    - the training data is a set of (_obs_, _action_) pairs:\n",
    "         - _obs_ is a game board where 1 is the active player and -1 is the oppponent\n",
    "         - _action_ is the column that a professional player will choose to play in _obs_\n",
    "    - Our supervised learning task: given an '*obs*' (game board), predict '*action*' (classification)\n",
    "    - To know more about the dataset creation, refer to '*src/data/part1_dataset_generator.ipynb*'\n",
    "    - the dataset used here can be found in: '*src/data/part1_data/part1_supervised_learning_data.txt*'\n",
    "<br> \n",
    "- **DATA PREPROCESSING:**\n",
    "    - To understand our implementation refer to *'src/models/custom_network.py'*\n",
    "    - the state of the environment is a _6 x 7_ board with values: {0: empty, 1: player, -1: opponent}\n",
    "    - The input of the model is a onehot encoded version of shape *2 x 6 x 7*\n",
    "        - first channel: {1: player, 0: empty or opponent}\n",
    "        - second channel: {1: opponent, 0: empty or player}\n",
    "    - For each channel, the empty positions that can be filled in the current turn are set to -1\n",
    "<br>\n",
    "- **NETWORK ARCHITECTURE:**\n",
    "    - To understand our implementation refer to *'src/models/custom_network.py'*\n",
    "    - We have implemented a basic network structure that allows different degrees of complexity\n",
    "    - The number of trainable parameters indicate the complexity of the network\n",
    "    - The name of this general architecture is _'CNET\\<*N*\\>_', where 'N' is the number of convolutional filters and hidden units\n",
    "    - **!!!!!** We implemented a **two-headed network achitecture** because some of the Deep Reinforcement Learning algorithms that we will implement later require two different prediction heads and are based on the network trained here. However, **the second head is NOT trained for this supervised learning task, only the first head**.\n",
    "    - The kernel sizes (4x4 and 2x2) and the size of the output (7) are fixed (do not depend on '*N*')\n",
    "    - For instance, the hidden layers of '_CNET*128*_' are:\n",
    "        - 2Dconvolutional layer with *128* 4x4 filters  (backbone network)\n",
    "        - 2Dconvolutional layer with *128* 2x2 filters  (backbone network)\n",
    "        - fully connected layer with *128* units   (backbone network)\n",
    "        - fully connected layer with *128* units (first prediction head) -> outputs the POLICY\n",
    "        - [NOT TRAINED HERE] fully connected layer with *128* units (second prediction head)\n",
    "    - For the general case _'CNET\\<*N*\\>_', change the number '128' to \\<N\\>\n",
    "<br>\n",
    "- **TRAINING STEPS:**\n",
    "    - Load the 200k data pairs (_obs, action_) from '*src/data/part1_data/part1_supervised_learning_data.txt*'\n",
    "    - trainig-validation-test data split: 160k + 20k + 20k samples (respectively)\n",
    "    - The following training parameters are the same for the all the networks:\n",
    "        - number of epochs = 20\n",
    "        - batch size = 64\n",
    "        - learning rate = 5e-4\n",
    "        - Loss function = Cross Entropy Loss\n",
    "        - weight decay (L2 regularization) = 2e-3\n",
    "    - Every 600 updates, we evaluate the model on the validation data\n",
    "    - When the training ends, we evaluate the model on the test data\n",
    "         \n",
    "- **RESULTS**:\n",
    "    - We used the network architecture 'CNET128' (defined above and in the code as well)\n",
    "    - After training for 20 epochs and using the training hyper-parameter values described above:\n",
    "        - ~87% training accuracy\n",
    "        - ~85% validation accuracy\n",
    "        - ~85% test accuracy \n",
    "        - minimum overfitting\n",
    "    - At the end of the training loop, the training and validation accuracies and losses are plotted \n",
    "    - The architecture CNET128 is saved in '*src/models/architectures/cnet128.json*'\n",
    "    - The best weights are saved in: '*src/models/saved_models/supervised_cnet128.pt*'\n",
    "    - The training hyper-parameters are saved in: '*src/models/saved_models/supervised_cnet128_hparams.json*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import BatchSampler, SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR PATH HERE\n",
    "code_dir = '/home/marc/Escritorio/RL-connect4/'\n",
    "\n",
    "if os.path.isdir(code_dir):\n",
    "    # local environment\n",
    "    os.chdir(code_dir)\n",
    "    print(f\"directory -> '{code_dir }'\")\n",
    "else:\n",
    "    # google colab environment\n",
    "    if os.path.isdir('./src'):\n",
    "        print(\"'./src' dir already exists\")\n",
    "    else:  # not unzipped yet\n",
    "        !unzip -q src.zip\n",
    "        print(\"'./src.zip' file successfully unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.custom_network import CustomNetwork\n",
    "from src.environment.connect_game_env import ConnectGameEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # where the data is located\n",
    "    'data_file_path': './src/data/part1_data/part1_supervised_learning_data.txt',\n",
    "    \n",
    "    # network architecture\n",
    "    'network_arch': {\n",
    "        'conv_block': [[128,4,0], 'relu', [128,2,0], 'relu'],\n",
    "        'fc_block': [128, 'relu'],\n",
    "        'first_head': [128, 'relu', 7],\n",
    "        'second_head': [128, 'relu', 1]\n",
    "    },\n",
    "    'model_name': 'cnet128',\n",
    "\n",
    "    # train-test split (sizes)\n",
    "    'train_size': 160000,\n",
    "    'val_size': 20000,\n",
    "    'test_size': 20000,\n",
    "\n",
    "    # Training params\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'loss_log_every': 200,\n",
    "    'validation_every': 600,\n",
    "    'weight_decay': 2e-3,\n",
    "    'lr': 5e-4,\n",
    "    \n",
    "    # save models\n",
    "    'save_model': True,  # debug mode\n",
    "    'save_model_file_path': './src/models/saved_models/supervised_{model_name}.pt'\n",
    "}\n",
    "\n",
    "\n",
    "hparams['save_model_file_path'] = hparams['save_model_file_path'].format(model_name=hparams['model_name'])\n",
    "hparams['n_samples'] = hparams['train_size'] + hparams['val_size'] + hparams['test_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNetwork(**hparams['network_arch']).to(device)\n",
    "print(summary(model, input_size=model.input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=hparams['lr'],\n",
    "    weight_decay=hparams['weight_decay']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "sep = ';'\n",
    "\n",
    "with open(hparams['data_file_path'], 'r') as file:\n",
    "    lines = file.read().split(sep)[:-1]  # last line is ''\n",
    "assert hparams['n_samples'] <= len(lines), f'not enough data: {hparams[\"n_samples\"]} > {len(lines)}'\n",
    "\n",
    "obs_list, actions_list = [], []\n",
    "for line in lines[:hparams['n_samples']]:\n",
    "    flat_obs = [int(i) for i in line[:-1]]\n",
    "    obs = np.array(flat_obs).reshape((6,7))\n",
    "    obs[obs==2] = -1\n",
    "    obs_list.append(obs)\n",
    "    actions_list.append(int(line[-1]))\n",
    "\n",
    "print(f\"{len(obs_list)} data samples loaded from '{hparams['data_file_path']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the data points into pytorch tensors to feed the model\n",
    "\n",
    "obs_tensor = torch.cat([model.obs_to_model_input(obs=o) for o in obs_list])\n",
    "actions_tensor = torch.tensor(actions_list, dtype=torch.long)\n",
    "\n",
    "# train-val-test split\n",
    "data_idx = random.sample(range(len(obs_list)), k=hparams['n_samples'])\n",
    "train_idx = data_idx[:hparams['train_size']]\n",
    "val_idx = data_idx[hparams['train_size']:-hparams['test_size']]\n",
    "test_idx = data_idx[-hparams['test_size']:]\n",
    "\n",
    "# train_x and train_y will be moved to device in batches while training\n",
    "train_x = obs_tensor[train_idx]\n",
    "train_y = actions_tensor[train_idx]\n",
    "\n",
    "val_x = obs_tensor[val_idx].to(device)\n",
    "val_y = actions_tensor[val_idx].to(device)\n",
    "\n",
    "test_x = obs_tensor[test_idx].to(device)\n",
    "test_y = actions_tensor[test_idx].to(device)\n",
    "\n",
    "print(f'train: {train_x.shape},  {train_y.shape}')\n",
    "print(f'val: {val_x.shape},  {val_y.shape}')\n",
    "print(f'test:  {test_x.shape},  {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output: torch.tensor, labels: torch.tensor):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the given predictions\n",
    "    \n",
    "    :param output: network output (predictions)\n",
    "    :param labels: ground truth\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "    acc = torch.sum(pred_y == labels).item() / float(labels.size(0))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def validate():\n",
    "    \"\"\"\n",
    "    Validate the model on the validation data: global val_x and val_y\n",
    "    Returns the valiation loss and validation accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    is_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pred, _ = model(val_x)\n",
    "        val_loss = loss_func(val_pred, val_y)\n",
    "        val_acc = accuracy(output=val_pred, labels=val_y)\n",
    "    \n",
    "    if is_training:\n",
    "        model.train()\n",
    "    \n",
    "    return val_loss.item(), val_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    Test the model on the test data: global test_x and test_y\n",
    "    Returns the test loss and test accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    is_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pred, _ = model(test_x)\n",
    "        test_loss = loss_func(test_pred, test_y)\n",
    "        test_acc = accuracy(output=test_pred, labels=test_y)\n",
    "    \n",
    "    if is_training:\n",
    "        model.train()\n",
    "    \n",
    "    return test_loss.item(), test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'loss': [], 'acc': [], \n",
    "           'val_loss': [], 'val_acc': [],\n",
    "           'test_loss': 0, 'test_acc': 0}\n",
    "\n",
    "env = ConnectGameEnv()\n",
    "\n",
    "model.train()\n",
    "\n",
    "num_epoch_steps = int(np.ceil(len(train_x)/hparams['batch_size']))\n",
    "step_count = 0\n",
    "for epoch in range(hparams['num_epochs']):\n",
    "    \n",
    "    epoch_step = 0\n",
    "    index_list = SubsetRandomSampler(range(len(train_x)))\n",
    "    for batch_index in BatchSampler(index_list, hparams['batch_size'], False):\n",
    "\n",
    "        batch_x = train_x[batch_index].to(device)\n",
    "        batch_y = train_y[batch_index].to(device)\n",
    "\n",
    "        batch_pred, _ = model(batch_x)\n",
    "        loss = loss_func(batch_pred, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        history['loss'].append(loss.item())\n",
    "        history['acc'].append(accuracy(batch_pred.detach(), batch_y))\n",
    "\n",
    "        step_count += 1\n",
    "        epoch_step += 1\n",
    "\n",
    "        if step_count % hparams['loss_log_every'] == 0:\n",
    "            avg_loss = float(np.mean(history['loss'][-hparams['loss_log_every']]))\n",
    "            avg_acc = float(np.mean(history['acc'][-hparams['loss_log_every']]))\n",
    "            print(f'Epoch: {epoch+1}/{hparams[\"num_epochs\"]},    ' +\n",
    "                  f'{epoch_step}/{num_epoch_steps} steps,    ' +\n",
    "                  f'avg_loss={round(avg_loss, 3)},    ' +\n",
    "                  f'avg_acc={round(avg_acc, 3)}')\n",
    "\n",
    "        if step_count % hparams['validation_every'] == 0:\n",
    "            val_loss, val_acc = validate()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            print(f'  --> val_loss={round(val_loss, 3)},   val_acc={round(val_acc, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the final model\n",
    "\n",
    "test_loss, test_acc = test()\n",
    "history['test_loss'] = test_loss\n",
    "history['test_acc'] = test_acc\n",
    "print(f'test_loss: {round(test_loss, 4)}')\n",
    "print(f'test_acc: {round(test_acc, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final model\n",
    "\n",
    "if hparams['save_model']:\n",
    "    model.save_weights(\n",
    "        file_path=hparams['save_model_file_path'],\n",
    "        training_hparams=hparams,\n",
    "    )\n",
    "    print('model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, m):\n",
    "    avg_data = []\n",
    "    for i in range(len(data) - m):\n",
    "        avg_data.append(np.mean(data[i:i+m]))\n",
    "    return avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "moving = 50\n",
    "\n",
    "max_val_x = len(history['val_loss']) * hparams['validation_every'] \n",
    "val_x_range = range(0, max_val_x, hparams['validation_every'])\n",
    "\n",
    "avg_loss = moving_average(history['loss'], m=moving)\n",
    "\n",
    "ax0.plot(range(len(avg_loss)), avg_loss, label='training loss')\n",
    "ax0.plot(val_x_range, history['val_loss'], label='validation loss')\n",
    "ax0.legend()\n",
    "ax0.set_xlabel('updates')\n",
    "ax0.set_ylabel('loss')\n",
    "ax0.set_title('Training and Validation Losses')\n",
    "\n",
    "avg_acc = moving_average(history['acc'], m=moving)\n",
    "ax1.plot(range(len(avg_acc)), avg_acc, label='training acc')\n",
    "ax1.plot(val_x_range, history['val_acc'], label='validation acc')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('updates')\n",
    "ax1.set_ylabel('acc')\n",
    "ax1.set_title('Training and Validation Accuracies')\n",
    "ax1.axhline(y=1, linestyle='--', alpha=0.3, color='black')\n",
    "ax1.set_ylim(-0.1, 1.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
